ğŸ”¡ Comparative Study: Deep Learning Models on Masked Morse Code vs Human Language
This project presents a comparative study of deep learning architectures â€” specifically Bidirectional RNNs, GRUs, and Transformers â€” to understand how well they model masked sequences in two different symbolic systems:

Morse Code â€” a dot-dash symbolic language

Human Language â€” English sentences with masked words

The focus is on evaluating these modelsâ€™ ability to recover masked tokens and generalize across symbolic vs natural language domains.

ğŸ“ Project Files
css
Copy
Edit
ğŸ“¦ Morse-Code-Model-Study
â”œâ”€â”€ Morse_Code_Bidirectional_GRU.ipynb
â”œâ”€â”€ Morse_Code_Bidirectional_RNN.ipynb
â”œâ”€â”€ Morse_Code_Transformer.ipynb
â”œâ”€â”€ Masked_morse_code_dataset.csv
â”œâ”€â”€ Masked NLP Dataset.csv
â””â”€â”€ README.md
ğŸ¯ Objective
To perform a comparative analysis of deep learning models on:

Learning and recovering masked sequences in symbolic (Morse) vs natural (English) language

Measuring how each model handles:

Sequence continuity and masking

Symbolic sparsity vs natural redundancy

Generalization, attention, and memory capacity

ğŸ§  Models Implemented
Model	Type	Application
Bidirectional RNN	Recurrent	Simple Morse / NLP sequence modeling
Bidirectional GRU	Recurrent (GRU)	Efficient pattern capture
Transformer	Attention-based	Long-range context & masking

ğŸ“Š Datasets
ğŸŸ¡ Masked_morse_code_dataset.csv
Dot-dash Morse sequences with [MASK] tokens

Used to evaluate symbolic token recovery

ğŸŸ£ Masked NLP Dataset.csv
English sentences with one or more masked words

Used to mirror BERT-style masking and measure transferability

ğŸ”¬ Key Research Questions
How do different architectures perform on symbolic vs natural language?

Can attention-based models better handle sparse, rule-based sequences like Morse?

How much context is needed to recover a masked token in each domain?

Do models generalize differently depending on domain structure?

ğŸ§¬ Model Design: Structuring the Morse Code Transformer
The Transformer model was built from scratch and tailored to the properties of Morse code:

âœ… Custom Tokenization
Morse sequences are split by spaces into individual symbols (e.g., ., -, /)

A vocabulary is built including [MASK] and [PAD]

âœ… MLM-Style Dataset
True target token is masked

Additional random masks are applied (~15% total masking)

Output labels are set to -100 for non-masked positions to ignore in loss

âœ… Architecture Parameters
Embedding Dimension: 256

Attention Heads: 8

Hidden Size: 512

Transformer Layers: 4

Dropout: 0.1

Max Sequence Length: 128

Optimized with CrossEntropyLoss and EarlyStoppingCallback

âœ… Training Setup
Batch Size: 32

Epochs: 50

Learning Rate: 3e-4

Device: GPU (if available)

ğŸ Getting Started
Installation
bash
Copy
Edit
pip install torch transformers pandas numpy scikit-learn
Running the Code
Open each notebook (RNN / GRU / Transformer)

Choose the appropriate dataset (Masked_morse_code_dataset.csv or Masked NLP Dataset.csv)

Run all cells to train and evaluate

ğŸš€ Future Work
Extend to multi-mask recovery or span prediction

Incorporate causal decoding for generation tasks

Study cross-domain transfer: can training on NLP help Morse?

Visualize attention weights for symbolic alignment

âœï¸ Author
Urja Damodhar
Machine Learning Research Assistant, Boston University
Portfolio â€¢ LinkedIn

